{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_PATH_NAME = \"Code-Phi-3-mini-128k-instruct-GGUF/\"  # folder to store the result in\n",
        "\n",
        "QUANT_PATH = './quants/'\n",
        "MERGES_PATH = './merges/'\n",
        "FULL_GGUF_PATH = './full_gguf/'\n",
        "\n",
        "FILE_NAME = OUTPUT_PATH_NAME.replace('/', '')\n",
        "FULL_FILE_NAME = FILE_NAME + '.gguf'\n",
        "OUTPUT_PATH_FULL_GGUF = FULL_GGUF_PATH + FULL_FILE_NAME\n",
        "OUTPUT_PATH_QUANT = QUANT_PATH + OUTPUT_PATH_NAME\n",
        "OUTPUT_PATH_MERGES = MERGES_PATH + OUTPUT_PATH_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MERGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/cg123/mergekit.git\n",
        "%cd mergekit\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\n",
        "CONFIG_YML = \"mergekit.yml\"  # merge configuration file\n",
        "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
        "LAZY_UNPICKLE = False  # experimental low-memory model loader\n",
        "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# actually do merge\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "from mergekit.config import MergeConfiguration\n",
        "from  mergekit.merge import MergeOptions, run_merge\n",
        "\n",
        "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
        "    y = yaml.safe_load(fp)\n",
        "    merge_config = MergeConfiguration.model_validate(y)\n",
        "\n",
        "run_merge(\n",
        "    merge_config,\n",
        "    out_path=OUTPUT_PATH_MERGES,\n",
        "    options=MergeOptions(\n",
        "        lora_merge_cache=LORA_MERGE_CACHE,\n",
        "        cuda=torch.cuda.is_available(),\n",
        "        copy_tokenizer=COPY_TOKENIZER,\n",
        "        lazy_unpickle=LAZY_UNPICKLE,\n",
        "        low_cpu_memory=LOW_CPU_MEMORY,\n",
        "        allow_crimes=True\n",
        "    ),\n",
        ")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(OUTPUT_PATH_MERGES)\n",
        "s = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{s/(1e9):.2f}B parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_PATH_MERGES)\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QUANTIZE\n",
        "Set IN_PATH if you just want to quantize. If you merge then quantize then leave it empty. Set LLAMA_3 to True if you are quintizing Llama 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Edit these if needed\n",
        "QUANT_SIZES = [ 'Q2_K', \n",
        "                'Q3_K_L', 'Q3_K_M', 'Q3_K_S',\n",
        "                'Q4_K_M', 'Q4_K_S',\n",
        "                'Q5_K_M', 'Q5_K_S',\n",
        "                'Q6_K',\n",
        "                'Q8_0'\n",
        "                ]\n",
        "\n",
        "IN_PATH = 'Phi-3-mini-code-finetune-128k-instruct-v1/'#OUTPUT_PATH_NAME#'Llama-3-14B-Instruct-v1.gguf'\n",
        "LLAMA_3 = False\n",
        "WAVECODER = True\n",
        "\n",
        "if IN_PATH != '':\n",
        "    OUTPUT_PATH_MERGES = IN_PATH\n",
        "\n",
        "if not os.path.exists(QUANT_PATH):\n",
        "    os.mkdir(QUANT_PATH)\n",
        "if not os.path.exists(MERGES_PATH):\n",
        "    os.mkdir(MERGES_PATH)\n",
        "if not os.path.exists(FULL_GGUF_PATH):\n",
        "    os.mkdir(FULL_GGUF_PATH)\n",
        "if not os.path.exists(OUTPUT_PATH_QUANT):\n",
        "    os.mkdir(OUTPUT_PATH_QUANT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(OUTPUT_PATH_MERGES)\n",
        "print(OUTPUT_PATH_FULL_GGUF)\n",
        "print(f'./llama.cpp/convert-hf-to-gguf.py {OUTPUT_PATH_MERGES} --outfile {OUTPUT_PATH_FULL_GGUF}')\n",
        "\n",
        "\n",
        "if not os.path.isfile(OUTPUT_PATH_FULL_GGUF):\n",
        "    !./llama.cpp/convert-hf-to-gguf.py {OUTPUT_PATH_MERGES} --outfile {OUTPUT_PATH_FULL_GGUF}\n",
        "else:\n",
        "    print('File already exists')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "QUANT_PRE_NAME = OUTPUT_PATH_QUANT + FILE_NAME\n",
        "for quant_size in QUANT_SIZES:\n",
        "    quant_out_name = QUANT_PRE_NAME + f'-{quant_size}.gguf'\n",
        "    if not os.path.isfile(quant_out_name):\n",
        "        !./llama.cpp/quantize {OUTPUT_PATH_FULL_GGUF} {quant_out_name} {quant_size}\n",
        "        if LLAMA_3: # fixes the end of string character\n",
        "            !./llama.cpp/gguf-py/scripts/gguf-set-metadata.py --force {quant_out_name} tokenizer.ggml.eos_token_id 128009\n",
        "    else:\n",
        "        print(f'File already exists: {quant_out_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test quant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GGUF_SIZE = 'Q4_K_M'\n",
        "llama_cpp_path = './llama.cpp'\n",
        "\n",
        "\n",
        "gguf_name = QUANT_PRE_NAME + f'-{GGUF_SIZE}.gguf'\n",
        "!{llama_cpp_path}/main -m {gguf_name} -p \"Penguins live in\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quant info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python {llama_cpp_path}/gguf-py/scripts/gguf-dump.py {gguf_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OLLAMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "QUANT_SIZE = 'Q4_K_M'\n",
        "OLLAMA_NAME = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if OLLAMA_NAME == '':\n",
        "    OLLAMA_NAME = FILE_NAME \n",
        "OLLAMA_IN_NAME = OLLAMA_NAME + f':{QUANT_SIZE}'\n",
        "!ollama create {OLLAMA_IN_NAME} -f ./Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelfile template\n",
        "\n",
        "FROM [Model-name]\n",
        "\n",
        "TEMPLATE \"\"\"\n",
        "\n",
        "[Model-Template]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PARAMETER [any parameter needed]\n",
        "\n",
        "LICENSE \"\"\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FROM [Model-name]\n",
        "\n",
        "TEMPLATE \"\"\"\n",
        "\n",
        "[Model-Template]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PARAMETER [any parameter needed]\n",
        "\n",
        "LICENSE \"\"\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UPLOAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HF_REPO_NAME = 'Phi-3-mini-code-finetune-128k-instruct-v1'\n",
        "MODEL_PATH = 'Phi-3-mini-code-finetune-128k-instruct-v1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed\n",
        "\n",
        "import gc\n",
        "\n",
        "del model, tokenizer\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!huggingface-cli login --token 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\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Upload model\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
        "\n",
        "s = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{s/(1e9):.2f}B parameters')\n",
        "\n",
        "model.push_to_hub(HF_REPO_NAME, use_temp_dir=False)\n",
        "\n",
        "# Upload tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "tokenizer.push_to_hub(HF_REPO_NAME, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BENCHMARK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modelfile_gguf = './Phi-3-mini-4k-instruct-q4.gguf'\n",
        "Modelfile_full = 'microsoft/Phi-3-mini-128k-instruct'\n",
        "prepare_path = '/home/me/git/can-ai-code/results/prepare_junior-v2_python-javascript_chatml-v2.ndjson'\n",
        "llama_ccp_path = '/home/me/AI/llama.cpp/main'\n",
        "param = '/home/me/git/can-ai-code/params/topk1.json'\n",
        "can_ai_code_path = '/home/me/git/can-ai-code'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python {can_ai_code_path}/prepare.py --interview {can_ai_code_path}/junior-v2/ --template {can_ai_code_path}/prompts/chatml-v2.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python interview-llamacpp.py f'{prepare_path}' f'{Modelfile_gguf}' f'{param}' --main f'{llama_ccp_path}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python compare.py {can_ai_code_path}/compare-v1/compare-llama2-coders.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## fp32/16 transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python interview_cuda.py './results/prepare_junior-v2_python-javascript_chatml-v2.ndjson' f'{param}' f'{Modelfile_full}' 'transformers' --quant='fp4'\n",
        "\n",
        "!python compare.py /home/me/git/can-ai-code/compare-v1/compare-llama2-coders.yaml\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
